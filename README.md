# Awesome-UQ-in-LLMs# Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey
:fire: Welcome to our Awesome-llm-uq repository. Our servey is accepted to KDD 2025 and we will continue to add the new paper from different venues. Please visit [Our Official Tutorial Website](https://xiao0o0o.github.io/2025KDD_tutorial/) for more information.

## Input Uncertainty :arrow_right:

- **[2024/02] Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling** _ICML2024_ [[paper]](https://proceedings.mlr.press/v235/hou24b.html?trk=public_post_comment-text) [[code]](https://github.com/UCSB-NLP-Chang/llm_uncertainty)

- **[2023/12] Uncertainty Quantification for In-Context Learning of Large Language Models** _NAACL2024_ [[paper]](https://aclanthology.org/2024.naacl-long.184.pdf) [[code]](https://github.com/lingchen0331/UQ_ICL)

- **[2023/11] Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering** _CVPR2024_ [[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Consistency_and_Uncertainty_Identifying_Unreliable_Responses_From_Black-Box_Vision-Language_Models_CVPR_2024_paper.pdf)

- **[2023/10] SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models** _EACL2024_ [[paper]](https://aclanthology.org/2024.eacl-long.143.pdf) [[code]](https://github.com/intuit-ai-research/SPUQ)


##  Reasoning Uncertainty :brain:

- **[2025/05] Token-Level Uncertainty Estimation for Large Language Model Reasoning** _arXiv_ [[paper]](https://arxiv.org/pdf/2505.11737)

- **[2025/02] CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought** _arXiv_ [[paper]](https://arxiv.org/pdf/2502.17214) [[code]](https://github.com/ZBox1005/CoT-UQ)

- **[2025/02] Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology** _arXiv_ [[paper]](https://arxiv.org/pdf/2502.17026)

- **[2024/06] Cycles of Thought: Measuring LLM Confidence through Stable Explanations** _arXiv_ [[paper]](https://arxiv.org/pdf/2406.03441)

- **[2024/02] Reasoning in Flux:Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance** _ACL2024_ [[paper]](https://aclanthology.org/2024.acl-long.131.pdf)

- **[2023/09] Tree of Uncertain Thoughts Reasoning for Large Language Models** _ICASSP2024_ [[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448355&casa_token=1JsyUV8y7ywAAAAA:CnK-NhZKt2MC8w5UyigmVIi0jrFrr2FdLXdzzqBL78aTmAKOQ1pzofsl1MoxZQDWKFD90i7W2g&tag=1)


## Parameter Uncertainty :gear:

- **[2025/05] Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models** _arXiv_ [[paper]](https://arxiv.org/pdf/2505.11731)

- **[2025/05] A Headto Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs** _arXiv_ [[paper]](https://arxiv.org/pdf/2505.08200)  [[code]](https://github.com/IINemo/llm-uncertainty-head)

- **[2024/06] Can LLMs Learn Uncertainty on Their Own? Expressing Uncertainty Effectively in A Self-Training Manner** _EMNLP2024_ [[paper]](https://aclanthology.org/2024.emnlp-main.1205.pdf) [[code]](https://github.com/NLP2CT/UaIT)

- **[2024/05] BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models** _NeurIPS2024_ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/7d53575463291ea6b5a23cf6e571f59b-Paper-Conference.pdf) [[code]](https://github.com/Wang-ML-Lab/bayesian-peft)

- **[2024/04] Uncertainty Estimation and Quantification for LLMs: A Simple Supervised Approach** _arXiv_ [[paper]](https://arxiv.org/pdf/2404.15993)  [[code]](https://github.com/LoveCatc/supervised-llm-uncertainty-estimation?tab=readme-ov-file)

- **[2024/02] Uncertainty quantification in fine-tuned LLMs using LoRA ensembles** _arXiv_ [[paper]](https://arxiv.org/pdf/2402.12264)  [[code]](https://github.com/prs-eth/LoRA-Ensemble)

- **[2023/10] Bayesian Low-rank Adaptation for Large Language Models** _ICLR2024_ [[paper]](https://openreview.net/pdf?id=FJiUyzOF1m) [[code]](https://github.com/MaximeRobeyns/bayesian_lora)

- **[2023/06] The Internal State of an LLM Knows When It's Lying** _EMNLP2023_  [[paper]](https://aclanthology.org/2023.findings-emnlp.68.pdf)


## Prediction Uncertainty :speech_balloon:	

- **[2025/05] Towards Harmonized Uncertainty Estimation for Large Language Models** _arXiv_ [[paper]](https://arxiv.org/pdf/2505.19073)

- **[2025/02] Uncertainty Quantification of Large Language Models through Multi-Dimensional Responses** _arXiv_ [[paper]](https://arxiv.org/pdf/2502.16820) 

- **[2024/10] KG-UQ: Knowledge Graph-Based Uncertainty Quantification for Long Text in Large Language Models** _WWW2025_ [[paper]](https://dl.acm.org/doi/pdf/10.1145/3701716.3717660)

- **[2024/07] LLM Uncertainty Quantification through Directional Entailment Graph and Claim Level Response Augmentation** _arXiv_ [[paper]](https://arxiv.org/pdf/2407.00994)

- **[2024/06] ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees** _EMNLP2024_ [[paper]](https://aclanthology.org/2024.findings-emnlp.404.pdf) [[code]](https://github.com/Zhiyuan-GG/Conformal-Uncertainty-Criterion/tree/main)

- **[2024/06] API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access** _EMNLP2024_ [[paper]](https://aclanthology.org/2024.findings-emnlp.54.pdf) [[code]](https://github.com/SU-JIAYUAN/LofreeCP)

- **[2024/06] LUQ:Long-text Uncertainty Quantification for LLMs** _EMNLP2024_ [[paper]](https://aclanthology.org/2024.emnlp-main.299.pdf) [[code]](https://github.com/caiqizh/LUQ)

- **[2024/05] Large Language Model Validity via Enhanced Conformal Prediction Methods** _NeurIPS2024_  [[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/d02ff1aeaa5c268dc34790dd1ad21526-Paper-Conference.pdf) [[code]](https://github.com/jjcherian/conformal-safety)

- **[2024/05] Kernel Language Entropy: Fine-grained Uncertainty Quantification for LLMs from Semantic Similarities** _NeurIPS2024_ [[paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/10c456d2160517581a234dfde15a7505-Paper-Conference.pdf) [[code]](https://github.com/AlexanderVNikitin/kernel-language-entropy)

- **[2024/03] Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models** _TMLR_ [[paper]](https://openreview.net/pdf?id=DWkJCSxKU5) [[code]](https://github.com/zlin7/UQ-NLG)

- **[2024/02] Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models** _ACL2024_ [[paper]](https://aclanthology.org/2024.acl-long.276.pdf) [[code]](https://github.com/jinhaoduan/SAR)

- **[2024/02] Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification** _ACL2024_ [[paper]](https://aclanthology.org/2024.findings-acl.558.pdf) [[code]](https://github.com/IINemo/lm-polygraph)

- **[2023/12] Uncertainty Estimation in Large Language Models to Support Biodiversity Conservation** _NAACL2024_ [[paper]](https://aclanthology.org/2024.naacl-industry.31.pdf) 

- **[2023/10] Conformal Language Modeling** _ICLR2024_ [[paper]](https://openreview.net/pdf?id=pzUhfQ74c5) [[code]](https://github.com/Varal7/conformal-language-modeling)

- **[2023/06] SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models** _EMNLP2023_ [[paper]](https://aclanthology.org/2023.emnlp-main.557.pdf) [[code]](https://github.com/potsawee/selfcheckgpt)

- **[2023/05] Conformal Prediction with Large Language Models for Multi-Choice Question Answering** _ICML 2023 (Neural Conversational AI TEACH) workshop_ [[paper]](https://arxiv.org/pdf/2305.18404) [[code]](https://github.com/bhaweshiitk/ConformalLLM/tree/main)

- **[2023/02] Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks** _ACL2023_ [[paper]](https://aclanthology.org/2023.acl-long.652.pdf) [[code]](https://github.com/AIRI-Institute/hybrid_uncertainty_estimation)

- **[2022/10] Out-of-Distribution Detection and Selective Generation for Conditional Language Models** _ICLR2023_ [[paper]](https://openreview.net/pdf?id=kJUS5nD0vPB)

- **[2022/10] Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation** _ICLR2023_ [[paper]](https://openreview.net/pdf?id=VD-AYtP0dve) [[code]](https://github.com/lorenzkuhn/semantic_uncertainty)

- **[2022/07] Language Models (Mostly) Know What They Know** _arXiv_ [[paper]](https://arxiv.org/pdf/2207.05221)

- **[2020/10] Uncertainty Estimation in Autoregressive Structured Prediction** _ICLR2021_ [[paper]](https://openreview.net/pdf?id=jN5y-zb5Q7m) [[code]](https://github.com/zlin7/UQ-NLG) [[code]](https://github.com/KaosEngineer/structured-uncertainty)


## Citation

If you find this repo and survey helpful for your research, please consider citing the following paper:

```
@article{liu2025uncertainty,
  title={Uncertainty quantification and confidence calibration in large language models: A survey},
  author={Liu, Xiaoou and Chen, Tiejin and Da, Longchao and Chen, Chacha and Lin, Zhen and Wei, Hua},
  journal={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2025}
}
```
